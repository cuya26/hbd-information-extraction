{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module contains utility stuff, like functions used to calculate scores and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nth_largest(position, my_list):\n",
    "    '''\n",
    "    This function gets the n-th maximum element from a list and the list with item above max removed\n",
    "    # TODO\n",
    "    '''\n",
    "    for i in range(position-1):\n",
    "        to_remove = max(my_list)\n",
    "        my_list.remove(to_remove)\n",
    "    return max(my_list), my_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_elements(my_list, indices):\n",
    "    '''\n",
    "    This function removes the elements in indices from my_list\n",
    "    INPUTS:\n",
    "    - my_list: list with elements to be removed\n",
    "    - indices: indeices of elements to remove\n",
    "    OUTPUTS:\n",
    "    - my_list: list cleaned\n",
    "    '''\n",
    "    for i in sorted(indices, reverse=True):\n",
    "        del my_list[i]\n",
    "    return my_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulate files with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load annotation files\n",
    "def get_annotations_file(basepath, file_format='json', sort=True):\n",
    "    from os import listdir\n",
    "    from os.path import join\n",
    "    '''\n",
    "    INPUTS:\n",
    "    - basepath: path containing the annotation files\n",
    "    - fmt: extention of files (default value: json)\n",
    "    - sort: boolean, specify if sorting files (default value: True)\n",
    "    OUTPUTS:\n",
    "    - all_annotations: list of path of annotation files\n",
    "    '''\n",
    "    files = []\n",
    "    for file in listdir(basepath):\n",
    "        if file.endswith(file_format):\n",
    "            files.append(join(basepath, file))\n",
    "            \n",
    "    if sort:\n",
    "        all_annotations = sorted(files)\n",
    "    else:\n",
    "        all_annotations = files\n",
    "    return all_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data from json and organized them in a more readable way\n",
    "def get_annotations_data(all_files):\n",
    "    import json\n",
    "    '''\n",
    "    INPUTS:\n",
    "    - file = path of the annotation file, format expected .json\n",
    "    OUTPUTS:\n",
    "    - triplets: list of dicts, every dict is the content of a file, it contains:\n",
    "        - 'context': text of document\n",
    "        - 'question_N': text of question index N\n",
    "        - 'question_N_ID': ID of question N\n",
    "        - 'answer_N': text of answer to question N\n",
    "    - paragraph_sort: \n",
    "    - all_questions: list of questions\n",
    "    '''\n",
    "    counter = 0\n",
    "    try:\n",
    "        triplets = []\n",
    "        for file in all_files:\n",
    "            \n",
    "            with open(file) as f:\n",
    "                whole = json.load(f)\n",
    "            \n",
    "            for i, doc in enumerate(whole['data']):\n",
    "                to_append = {}\n",
    "                for j, par in enumerate(doc['paragraphs']):\n",
    "                    # Extract paragraph\n",
    "                    paragraph = par['qas']\n",
    "                    # Sort paragraph by question index\n",
    "                    paragraph_sort = sorted(paragraph, key=lambda d: int(d['question'][:3])) \n",
    "                    to_append['context'] = par['context']\n",
    "                    for k, qas in enumerate(paragraph_sort):\n",
    "                        to_append[f'question_{counter}'] = qas['question'][4:]\n",
    "                        to_append[f'question_{counter}_ID'] = qas['id']\n",
    "                        try: # Answer exists\n",
    "                            to_append[f'answer_{counter}'] = qas['answers'][0]['text']\n",
    "                        except: # Answer does not exist\n",
    "                            to_append[f'answer_{counter}'] = ''\n",
    "                        counter+=1\n",
    "                triplets.append(to_append)\n",
    "        # Get questions\n",
    "        all_questions = []\n",
    "        counter = 0\n",
    "        for i, (k, v) in enumerate(triplets[0].items()):\n",
    "            if (k.startswith('question') and not k.endswith('ID')):\n",
    "                all_questions.append((counter, v))\n",
    "                counter+=1\n",
    "        return triplets, paragraph_sort, all_questions\n",
    "    except:\n",
    "        print('Error while loading data from file!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load annotation files\n",
    "def setup_pipeline(model_checkpoint, max_ans_len, handle_impossible_ans):\n",
    "    from transformers import pipeline, AutoTokenizer\n",
    "    '''\n",
    "    This function gets the model checkpoint and setup the pipeline\n",
    "    INPUTS:\n",
    "    - model_checkpoint: path to the model to use\n",
    "    - max_ans_len: max length in token of the answer\n",
    "    - handle_impossible_ans: True for SQuAD 2.0+ models\n",
    "    OUTPUTS:\n",
    "    - nlp_pipeline: NLP pipeline for the specified downstream task\n",
    "    '''\n",
    "    # Load Tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "    # Setup QA pipeline\n",
    "    nlp_pipeline = pipeline(\n",
    "        'question-answering',\n",
    "        model=model_checkpoint,\n",
    "        tokenizer=model_checkpoint,\n",
    "        max_answer_len = max_ans_len,\n",
    "        handle_impossible_answer = handle_impossible_ans,\n",
    "    )\n",
    "    return nlp_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elaborate statistical scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_EM_score(ref_list, pred_list, debugmode=False):\n",
    "    '''\n",
    "    This function gets two list of answers, and calculate the EM score between them.\n",
    "    INPUT:\n",
    "    - ref_list = list of reference answers\n",
    "    - pred_list = list of predicted answers. The number of elements must be the same.\n",
    "    OUTPUT:\n",
    "    - EM_score = EM score of all answers\n",
    "    - EM_score_not_empty = EM score only of not-empty answers\n",
    "    - EM_score_empty = EM score only of empty answers\n",
    "    '''\n",
    "    \n",
    "    # ALL Answers\n",
    "    tot = len(ref_list)\n",
    "    part = 0\n",
    "    for i, ref in enumerate(ref_list):\n",
    "        pred = pred_list[i]\n",
    "        if ref==pred:\n",
    "            part+=1\n",
    "    EM_score = part/tot\n",
    "    \n",
    "    # ONLY not empty Answers\n",
    "    ref_list_not_empty = list(filter(lambda ref_list: ref_list != '', ref_list))\n",
    "    pred_list_not_empty = []\n",
    "    for i, el in enumerate(ref_list):\n",
    "        if el!='':\n",
    "            pred_list_not_empty.append(pred_list[i])\n",
    "    tot_not_empty = len(ref_list_not_empty)\n",
    "    part_not_empty = 0\n",
    "    for i, ref in enumerate(ref_list_not_empty):\n",
    "        pred = pred_list_not_empty[i]\n",
    "        if ref==pred:\n",
    "            part_not_empty+=1\n",
    "    EM_score_not_empty = part_not_empty/tot_not_empty\n",
    "    \n",
    "    #TODO test and correct following code\n",
    "    # # ONLY empty Answers\n",
    "    # ref_list_empty = list(filter(lambda ref_list: ref_list == '', ref_list))\n",
    "    # # print(ref_list_not_empty)\n",
    "    # pred_list_empty = []\n",
    "    # for i, el in enumerate(ref_list):\n",
    "    #     # print(i, el)\n",
    "    #     if el=='':\n",
    "    #         pred_list_empty.append(pred_list[i])\n",
    "    # tot_empty = len(ref_list_empty)\n",
    "    # part_empty = 0\n",
    "    # for i, ref in enumerate(ref_list_empty):\n",
    "    #     pred = pred_list_empty[i]\n",
    "    #     if ref==pred:\n",
    "    #         part_empty+=1\n",
    "    # EM_score_empty = part_empty/tot_empty\n",
    "    \n",
    "    return EM_score, EM_score_not_empty#, EM_score_empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_EM_score_single_questions(ref_list, pred_list, debugmode=False):\n",
    "    '''\n",
    "    This function gets two list of answers, and calculate the EM score on single answers.\n",
    "    INPUT:\n",
    "    - ref_list = list of reference answers\n",
    "    - pred_list = list of predicted answers. The number of elements must be the same.\n",
    "    OUTPUT:\n",
    "    - EM_score_single = EM scores on single answers\n",
    "    '''\n",
    "    EM_score_single = []\n",
    "    tot = len(ref_list)\n",
    "    part = 0\n",
    "    for ref, pred in zip(ref_list, pred_list):\n",
    "        if ref==pred:\n",
    "            EM_score_single.append(1)\n",
    "        else:\n",
    "            EM_score_single.append(0)\n",
    "\n",
    "    return EM_score_single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_stat_param(TP, FP, FN):\n",
    "    '''\n",
    "    This function gets the number of True Positives, False Positives, and False Negatives, and calculates Precision, Recall, and F1-Score\n",
    "    INPUT:\n",
    "    - TP = number of True Positives\n",
    "    - FP = number of False Positives\n",
    "    - FN = number of False Negatives\n",
    "    OUTPUT:\n",
    "    - precision\n",
    "    - recall\n",
    "    - f1_score\n",
    "    '''\n",
    "    if (TP+FP)==0:\n",
    "#         precision = 'N/A'\n",
    "        precision = 0\n",
    "    else:\n",
    "        precision = TP/(TP+FP)\n",
    "    if (TP+FN)==0:\n",
    "#         recall = 'N/A'\n",
    "        recall = 0\n",
    "    else:\n",
    "        recall = TP/(TP+FN)\n",
    "    if ((TP+FP)==0) or ((TP+FN)==0) or ((precision+recall)==0):\n",
    "        # f1_score = 'N/A'\n",
    "        f1_score = 0\n",
    "    else:\n",
    "        f1_score = 2*precision*recall/(precision+recall)\n",
    "    return precision, recall, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_quest_param(ref_list, pred_list, model_checkpoint):\n",
    "    import collections\n",
    "    from transformers import AutoTokenizer\n",
    "    '''\n",
    "    This function gets two list of answers, and calculate the number of True Positives, False Positives, and False Negatives.\n",
    "    INPUT:\n",
    "    - ref_list = list of reference answers\n",
    "    - pred_list = list of predicted answers. The number of elements must be the same\n",
    "    - model_checkpoint = NLP model, used to tokenize texts\n",
    "    OUTPUT:\n",
    "    - all_TP = list that contains True Positives for every single answer\n",
    "    - all_FP = list that contains False Positives for every single answer\n",
    "    - all_FN = list that contains False Negatives for every single answer\n",
    "    '''\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "    all_TP, all_FP, all_FN = [], [], []\n",
    "    TP, FP, FN = 0, 0, 0\n",
    "    for ref, pred in zip(ref_list, pred_list):\n",
    "        ref_toks = tokenizer.encode(ref.strip('\\n'))[1:-1]\n",
    "        pred_toks = tokenizer.encode(pred.strip('\\n'))[1:-1]\n",
    "        common = collections.Counter(ref_toks) & collections.Counter(pred_toks)\n",
    "        TP = sum(common.values()) #  TP for specific question is number of shared tokens\n",
    "        all_TP.append(TP)\n",
    "        FP =  sum((collections.Counter(pred_toks) - collections.Counter(ref_toks)).values()) #  FPs are tokens in pred but not in ref\n",
    "        all_FP.append(FP)\n",
    "        FN =  sum((collections.Counter(ref_toks) - collections.Counter(pred_toks)).values()) #  FNs are tokens in ref but not in pred= len(ref_toks) - len(pred_toks) #  FNs are tokens in ref but not in pred\n",
    "        all_FN.append(FN)\n",
    "    return all_TP, all_FP, all_FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elaborate results by single question\n",
    "def elaborate_results_global(pred_ans_for_stats, correct_ans_for_stats, all_annotations, my_questions, pred_answers, model_checkpoint, show_answers=True):\n",
    "    from itertools import repeat\n",
    "    import statistics\n",
    "    import string\n",
    "    '''\n",
    "    This function get as input annotation files, questions, predicted answers and the model checkpoint, and\n",
    "    calculates the global statistical results.\n",
    "    INPUTS:\n",
    "    - pred_ans_for_stats: list of predicted answers\n",
    "    - correct_ans_for_stats: list of reference answers\n",
    "    - all_annotations: paths of the files with data\n",
    "    - my_questions: list of questions the model has answered\n",
    "    - model_checkpoint: model used to get answers, it is used for tokeninzing answers an thus calculate TPs, FP, and FNs\n",
    "    - show_answers: boolean, if True prints question VS reference answer VS predicted answer (default value: True)\n",
    "    OUTPUTS:\n",
    "    - EM_TOTAL: EM score calculated on every question\n",
    "    - EM_NOT_EMPTY_TOTAL: EM score calculated only on not empty questions\n",
    "    - P_TOTAL: Precision calculated on every question\n",
    "    - R_TOTAL: Recall calculated on every question\n",
    "    - F1_MACRO_TOTAL: F1 score calculated as the average of every single F1 scores\n",
    "    - F1_WEIGHTED_TOTAL: Precision calculated starting from TPs, FPs, and FNs of every single question\n",
    "    '''\n",
    "    # Create variables\n",
    "    EM_scores = [[] for i in repeat(None, len(all_annotations))] # List of EM scores\n",
    "    EM_score_not_emptys = [[] for i in repeat(None, len(all_annotations))] # List of EM scores only for NOT empty answers\n",
    "    EM_score_emptys = [[] for i in repeat(None, len(all_annotations))] # List of EM scores only for empty answers\n",
    "    all_TP_global = [[] for i in repeat(None, len(all_annotations))] # List of all TPs for every document\n",
    "    all_FP_global = [[] for i in repeat(None, len(all_annotations))] # List of all FPs for every document\n",
    "    all_FN_global = [[] for i in repeat(None, len(all_annotations))] # List of all FNs for every document\n",
    "                \n",
    "    # Compare correct answer with predicted one\n",
    "    if show_answers:\n",
    "        for i, (ref_list, pred_list) in enumerate(zip(correct_ans_for_stats, pred_ans_for_stats)):\n",
    "            print('#'*100)\n",
    "            print(f'Doc {all_annotations[i].split(\"/\")[-1]}')\n",
    "            for j, (ref, pred) in enumerate(zip(ref_list, pred_list)):\n",
    "                print('-'*50)\n",
    "                print(f'Question {my_questions[j][0]}: {my_questions[j][1]}')\n",
    "                print(f'Reference answer = \"{ref}\"')\n",
    "                print(f'Predicted answer = \"{pred}\"')\n",
    "                  \n",
    "    # Calculate EM Scores\n",
    "    for i, (correct_answers, pred_answers) in enumerate(zip(correct_ans_for_stats, pred_ans_for_stats)):\n",
    "        EM_scores[i], EM_score_not_emptys[i] = get_EM_score(correct_answers, pred_answers)\n",
    "    \n",
    "    # Calculate FPs, FNs, TPs\n",
    "    for i, (correct_answers, pred_answers) in enumerate(zip(correct_ans_for_stats, pred_ans_for_stats)):\n",
    "        all_TP_global[i], all_FP_global[i], all_FN_global[i] = get_all_quest_param(correct_answers, pred_answers, model_checkpoint)\n",
    "    F1_global = [[] for i in repeat(None, len(all_annotations))]\n",
    "    for i, (doc_TP, doc_FP, doc_FN) in enumerate(zip(all_TP_global, all_FP_global, all_FN_global)):\n",
    "        temp_FP = []\n",
    "        for j, (TP, FP, FN) in enumerate(zip(doc_TP, doc_FP, doc_FN)):\n",
    "            P, R, F1 = calculate_stat_param(TP, FP, FN)\n",
    "            temp_FP.append(F1)\n",
    "            F1_global[i] = temp_FP\n",
    "\n",
    "    # Aggregated results\n",
    "    EM_TOTAL = statistics.mean(EM_scores)\n",
    "    EM_NOT_EMPTY_TOTAL = statistics.mean(EM_score_not_emptys)\n",
    "    full_TP = sum([sum(i) for i in zip(*all_TP_global)])\n",
    "    full_FP = sum([sum(i) for i in zip(*all_FP_global)])\n",
    "    full_FN = sum([sum(i) for i in zip(*all_FN_global)])\n",
    "    F1_MACRO_TOTAL = statistics.mean([item for sublist in F1_global for item in sublist])\n",
    "    P_TOTAL, R_TOTAL, F1_WEIGHTED_TOTAL = calculate_stat_param(full_TP, full_FP, full_FN)\n",
    "               \n",
    "    results = {\n",
    "        'EM_TOTAL': EM_TOTAL,\n",
    "        'EM_NOT_EMPTY_TOTAL': EM_NOT_EMPTY_TOTAL,\n",
    "        'P_TOTAL': P_TOTAL,\n",
    "        'R_TOTAL': R_TOTAL,\n",
    "        'F1_MACRO_TOTAL': F1_MACRO_TOTAL,\n",
    "        'F1_WEIGHTED_TOTAL': F1_WEIGHTED_TOTAL,\n",
    "    }\n",
    "                  \n",
    "    return results, all_TP_global, all_FP_global, all_FN_global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elaborate results by single question\n",
    "def elaborate_results_by_question(all_TP_global, all_FP_global, all_FN_global):\n",
    "    import pandas as pd\n",
    "    '''\n",
    "    This function ...\n",
    "    INPUTS:\n",
    "    - \n",
    "    OUTPUTS:\n",
    "    - \n",
    "    '''\n",
    "    quest_num = len(all_TP_global[0])\n",
    "    TPs = [0] * quest_num\n",
    "    FPs = [0] * quest_num\n",
    "    FNs = [0] * quest_num\n",
    "    EMs = [0] * quest_num\n",
    "    EMs_not_empty = [0] * quest_num\n",
    "    not_empty = [0] * quest_num\n",
    "    \n",
    "    for i, stats in enumerate(zip(all_TP_global, all_FP_global, all_FN_global)):\n",
    "        tp, fp, fn = stats[0], stats[1], stats[2]\n",
    "        for j in range(quest_num):\n",
    "            TPs[j]+=tp[j]\n",
    "            FPs[j]+=fp[j]\n",
    "            FNs[j]+=fn[j]\n",
    "            if ((fp[j]==0) & (fn[j]==0)):\n",
    "                EMs[j]+=1 \n",
    "            # EM for not empty questions only\n",
    "            if ((tp[j]+fp[j]+fn[j])!=0):\n",
    "                not_empty[j]+=1\n",
    "                if ((fp[j]==0) & (fn[j]==0)):\n",
    "                    EMs_not_empty[j]+=1 \n",
    "            \n",
    "    index = []\n",
    "    for i in range(quest_num):\n",
    "        index.append(f'Q_{i}')        \n",
    "    \n",
    "    df_questions = pd.DataFrame(\n",
    "        index=index,\n",
    "        columns=['Precision', 'Recall', 'F1-score', 'EM', 'EM_not_empty']\n",
    "    )\n",
    "    counter = 0\n",
    "    for i in range(quest_num):\n",
    "        P, R, F1 = calculate_stat_param(TPs[counter], FPs[counter], FNs[counter])\n",
    "        try:\n",
    "            df_questions.loc[f'Q_{i}', 'Precision']=f'{P*100:.2f}'\n",
    "        except:\n",
    "            df_questions.loc[f'Q_{i}', 'Precision']='0'\n",
    "        try:\n",
    "            df_questions.loc[f'Q_{i}', 'Recall']=f'{R*100:.2f}'\n",
    "        except:\n",
    "            df_questions.loc[f'Q_{i}', 'Recall']='0'\n",
    "        try:\n",
    "            df_questions.loc[f'Q_{i}', 'F1-score']=f'{F1*100:.2f}'\n",
    "        except:\n",
    "            df_questions.loc[f'Q_{i}', 'F1-score']='0'\n",
    "        try:\n",
    "            em = EMs[counter]/len(all_TP_global)\n",
    "            df_questions.loc[f'Q_{i}', 'EM']=f'{em*100:.2f}'\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            em_ne = EMs_not_empty[counter]/not_empty[counter]\n",
    "            df_questions.loc[f'Q_{i}', 'EM_not_empty']=f'{em_ne*100:.2f}'\n",
    "        except:\n",
    "            pass\n",
    "        counter+=1\n",
    "    df_questions = df_questions.apply(pd.to_numeric)\n",
    "    \n",
    "    return df_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elaborate results by single document\n",
    "def elaborate_results_by_document(all_TP_global, all_FP_global, all_FN_global):\n",
    "    import pandas as pd\n",
    "    import statistics\n",
    "    '''\n",
    "    This function ...\n",
    "    INPUTS:\n",
    "    - \n",
    "    OUTPUTS:\n",
    "    - \n",
    "    '''\n",
    "    index = []\n",
    "    for i in range(len(all_TP_global)):\n",
    "        index.append(f'Doc_{i}')        \n",
    "\n",
    "    df_documents = pd.DataFrame(\n",
    "        index=index,\n",
    "        columns=['Precision', 'Recall', 'F1-score', 'EM', 'EM_not_empty']\n",
    "    )\n",
    "                                  \n",
    "    for i, stats in enumerate(zip(all_TP_global, all_FP_global, all_FN_global)):\n",
    "        TPs, FPs, FNs = stats[0], stats[1], stats[2]\n",
    "        P_list, R_list, F1_list, EM_list, EM_NE_list, not_empty = [], [], [], [], [], []\n",
    "        # Get P, R, F1, EM for every question\n",
    "        for tp, fp, fn in zip(TPs, FPs, FNs):\n",
    "            P, R, F1 = calculate_stat_param(tp, fp, fn)\n",
    "            P_list.append(P)\n",
    "            R_list.append(R)\n",
    "            F1_list.append(F1)\n",
    "            if ((fp==0) & (fn==0)):\n",
    "                EM_list.append(1)\n",
    "            else:\n",
    "                EM_list.append(0)\n",
    "            # EM for not empty questions only\n",
    "            if ((tp+fp+fn)!=0):\n",
    "                if ((fp==0) & (fn==0)):\n",
    "                    EM_NE_list.append(1)\n",
    "                else:\n",
    "                    EM_NE_list.append(0)\n",
    "        # Get P, R, F1 for every document by averaging single question ones\n",
    "        P_avg = statistics.mean(P_list)\n",
    "        R_avg = statistics.mean(R_list)\n",
    "        F1_avg = statistics.mean(F1_list)\n",
    "        EM_avg = statistics.mean(EM_list)\n",
    "        EM_NE_avg = statistics.mean(EM_NE_list)\n",
    "        df_documents.loc[f'Doc_{i}', 'Precision'] = f'{P_avg*100:.2f}'\n",
    "        df_documents.loc[f'Doc_{i}', 'Recall'] = f'{R_avg*100:.2f}'\n",
    "        df_documents.loc[f'Doc_{i}', 'F1-score'] = f'{F1_avg*100:.2f}'\n",
    "        df_documents.loc[f'Doc_{i}', 'EM'] = f'{EM_avg*100:.2f}'\n",
    "        df_documents.loc[f'Doc_{i}', 'EM_not_empty'] = f'{EM_NE_avg*100:.2f}'\n",
    "\n",
    "    df_documents = df_documents.apply(pd.to_numeric)\n",
    "                                  \n",
    "    return df_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Miscellanea (to test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split text in topics\n",
    "\n",
    "# # Define pipeline to check topic\n",
    "# classifier = pipeline(\n",
    "#     \"zero-shot-classification\",\n",
    "#     model=\"MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7\"\n",
    "# )\n",
    "# candidate_labels = [\n",
    "#     'dati sociodemografici',\n",
    "#     'storia clinica',\n",
    "#     'fattori di rischio',\n",
    "#     'comorbiditÃ  somatica',\n",
    "#     'esame obiettivo',\n",
    "#     'farmaci',\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split in topics before answering\n",
    "\n",
    "# for j, file in enumerate(all_annotations):\n",
    "#     data, paragraph = utility.get_annotations_data(file)\n",
    "#     for doc in data:\n",
    "#         # # Split text in chunks\n",
    "#         # text = doc['context']\n",
    "#         # split_list = text.split(sep='\\n \\n')\n",
    "#         # split_list_clean = []\n",
    "#         # for split in split_list:\n",
    "#         #     if len(split.strip())==0:\n",
    "#         #         pass\n",
    "#         #     else:\n",
    "#         #         split_list_clean.append(split)\n",
    "#         # # Check topics\n",
    "#         # split_topics = []\n",
    "#         # for split in split_list_clean:\n",
    "#         #     result = classifier(split, candidate_labels, multi_label=True)\n",
    "#         #     split_topics.append(result['labels'][0])\n",
    "#         # # Put together all split with same topic\n",
    "#         # context_by_topic = {}\n",
    "#         # for label in candidate_labels:\n",
    "#         #     to_keep = [i for i, x in enumerate(split_topics) if x==label]\n",
    "#         #     # Merge split with same topic\n",
    "#         #     text = ''\n",
    "#         #     for el in to_keep:\n",
    "#         #         text+=split_list_clean[el]\n",
    "#         #     context_by_topic[label] = text\n",
    "#         for i, q in my_questions:\n",
    "#             # print(q)\n",
    "#             # print(doc['context'][:20])\n",
    "#             # continue\n",
    "#             QA_input = {\n",
    "#                 'question': q,\n",
    "#                 'context': doc['context'] # Process complete text, TEST03\n",
    "#             }\n",
    "#             pred_answers[j].append((i, nlp(QA_input)))\n",
    "# end = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOT USED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_answers(contexts, questions, num_answers, model, tokenizer, debug=False, advancedDebug=False):\n",
    "#     '''\n",
    "#     This function gets answers for questions.\n",
    "#     Inputs:\n",
    "#     - contexts = list of contexts\n",
    "#     - questions = list of question, expressed as sentences in natural language\n",
    "#     - num_answers = number of possible answers to show\n",
    "#     - model = model to perform NLP\n",
    "#     - tokenizer = tokenizer to perform NLP\n",
    "#     Outputs:\n",
    "#     - Answer for every question\n",
    "#     - Probability for every answer\n",
    "    \n",
    "#     NOTE:\n",
    "#     Model is case-sensitive: \"Come\" is a different token from \"come\"\n",
    "#     Input of tokenizer is question + context\n",
    "#     [CLS] = Token that indicates the beginning of input\n",
    "#     [SEP] = Token that marks separation between question and context\n",
    "#     '''\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "#     for context in contexts:\n",
    "#         print(\"-\"*90)\n",
    "#         print(f'Context: \"{context}\"')\n",
    "#     for i, question in enumerate(questions):\n",
    "#         print(\"-\"*90)\n",
    "#         print(\"-\"*90)\n",
    "#         inputs = my_tokenizer(question, context, padding=True, truncation=True, return_tensors=\"pt\") # return_tensors=\"pt\" -> the returned tensor is in PyTorch format\n",
    "#         # inputs[\"input_ids\"] is a tensor containing ids of question + context\n",
    "#         # input_ids converts the tensor to list and extract the list itself\n",
    "#         input_ids = inputs[\"input_ids\"].tolist()[0] \n",
    "#         text_tokens = tokenizer.convert_ids_to_tokens(input_ids) # Very useful for debug!!!\n",
    "\n",
    "#         if debug:\n",
    "#             print('--Token numbers and respective word--')\n",
    "#             for j, id in enumerate(input_ids):\n",
    "#                 print(f'Token #{j} num = {id}, Token #{j} text = {text_tokens[j]}')\n",
    "\n",
    "#         # Extract outputs from model\n",
    "#         outputs = my_model(**inputs)\n",
    "#         # answer_start_scores is a tensor that contains a logit for every token\n",
    "#         # Such logits represent the probability that the token is the START of answer\n",
    "#         answer_start_scores = outputs.start_logits\n",
    "#         # answer_end_scores is a tensor that contains a logit for every token\n",
    "#         # Such logits represent the probability that the token is the END of answer\n",
    "#         answer_end_scores = outputs.end_logits\n",
    "\n",
    "#         if advancedDebug:\n",
    "#             print(torch.softmax(answer_start_scores, dim=1))\n",
    "#             print(torch.softmax(answer_end_scores, dim=1))\n",
    "\n",
    "#         # Get probabilities of START and END\n",
    "#         # answer_start_list = % for every token to be START of answer\n",
    "#         answer_start_list = torch.softmax(answer_start_scores, dim=1).tolist()[0]\n",
    "#         # answer_end_list = % for every token to be END of answer\n",
    "#         answer_end_list = torch.softmax(answer_end_scores, dim=1).tolist()[0]\n",
    "\n",
    "#         print(f'Question: \"{question}\"')\n",
    "#         # Print the most probable n answers, where n = num_answers\n",
    "#         for k in range(num_answers):\n",
    "#             # Answer START\n",
    "#             (start_prob, start_list) = nth_largest(k+1, answer_start_list) \n",
    "#             start_index = start_list.index(start_prob)\n",
    "#             # Answer END\n",
    "#             (end_prob, end_list) = nth_largest(k+1, answer_end_list) \n",
    "#             end_index = end_list.index(end_prob)+1\n",
    "#             answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[start_index:end_index]))\n",
    "#             print(\"-\"*30)\n",
    "#             print(f'Answer #{k+1}: \"{answer}\"')\n",
    "#             print(f'Start token probability #{k+1}: {start_prob*100:.2f}')\n",
    "#             print(f'End token probability #{k+1}: {end_prob*100:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
