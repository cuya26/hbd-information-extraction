{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "peT3_7NVr0IW",
    "tags": []
   },
   "source": [
    "### Setup notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qZTT0UtRsWQT"
   },
   "outputs": [],
   "source": [
    "# Utility libraries\n",
    "import collections\n",
    "from itertools import repeat\n",
    "import json\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pandas as pd\n",
    "import pdfplumber\n",
    "import statistics\n",
    "import string\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "# Graphic libraries\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# NLP libraries\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "\n",
    "# Custom libraries\n",
    "import my_utility as utility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YUggg9Ghv9Eb",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert path of file with answers, Haystack format\n",
    "# Change accordingly to your PC paths\n",
    "my_path = '/home/claudio/ClaudioCrema_stuff/HBD/hbd-information-extraction/Annotations/HBD_discharge_letters_docs/'\n",
    "all_annotations = [my_path+f for f in listdir(my_path) if isfile(join(my_path, f))]\n",
    "all_annotations = sorted(all_annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J0MW905SsmdP"
   },
   "outputs": [],
   "source": [
    "# Model from HuggingFace\n",
    "# model_checkpoint = 'luigisaetta/squad_it_xxl_cased_hub1'\n",
    "# model_checkpoint = 'mrm8488/bert-italian-finedtuned-squadv1-it-alfa'\n",
    "model_checkpoint = 'deepset/xlm-roberta-large-squad2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J0MW905SsmdP"
   },
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wjMJJywhKZYs"
   },
   "outputs": [],
   "source": [
    "# Setup QA pipeline\n",
    "nlp = pipeline(\n",
    "    'question-answering',\n",
    "    model=model_checkpoint,\n",
    "    tokenizer=model_checkpoint,\n",
    "    max_anwer_len = 30,\n",
    "    handle_impossible_answer = True # Only for squad2 models\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2he-5F8WwTc0",
    "tags": []
   },
   "source": [
    "### Get answers - Italian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get questions\n",
    "all_questions = []\n",
    "counter = 0\n",
    "data, paragraph = utility.get_annotations_data(all_annotations[0]) # Read first annotation file, to get list of questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (k, v) in enumerate(data[0].items()):\n",
    "    if (k.startswith('question') and not k.endswith('ID')):\n",
    "        all_questions.append((counter, v))\n",
    "        counter+=1\n",
    "        \n",
    "my_questions = all_questions\n",
    "# Remove sex related question (it requires an inference)\n",
    "my_questions.pop(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred_answers = [[] for i in repeat(None, len(all_annotations))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Split text in topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define pipeline to check topic\n",
    "# classifier = pipeline(\n",
    "#     \"zero-shot-classification\",\n",
    "#     model=\"MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7\"\n",
    "# )\n",
    "# candidate_labels = [\n",
    "#     'dati sociodemografici',\n",
    "#     'storia clinica',\n",
    "#     'fattori di rischio',\n",
    "#     'comorbidità somatica',\n",
    "#     'esame obiettivo',\n",
    "#     'farmaci',\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_text = '''L’ esame obiettivo neurologico all’ingresso evidenziava: \\nPaziente vigile, collaborante, orientato nello spazio e nel tempo; deficit della memoria anterograda, linguaggio con qualche difficoltà nel \\ntrovare le parole, a tratti ripetitivo. Funzioni prassiche nella norma.  \\nNervi cranici: campo visivo integro al test del confronto tra quadranti, non deficit oculomozione, rimanenti nervi cranici nella norma. \\nArti superiori: Stenia nella norma. Non deficit del trofismo muscolare. Tono nella norma. Non bradicinesia. \\nArti inferiori: Stenia nella norma. Non deficit del trofismo muscolare. Tono nella norma. \\nRiflessi Osteotendinei: Arti SUP: normoevocabili;  Arti INF: normoevocabili; Riflesso da Stimolazione Cutaneo Plantare in flessione \\nbilateralmente. Assenti muso, glabella e palmomentoniero. Hoffmann assente bilat. \\nMovimenti involontari/Tremore: assenti. \\n \\n     \\n   \\n \\n \\n \\nCoordinazione: nei limiti di norma. \\nSensibilità: nei limiti di norma. \\nStazione eretta: nella norma. \\nDeambulazione: nella norma. \\nNon ipotensione ortostatica.'''\n",
    "# # print(my_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "for j, file in enumerate(all_annotations):\n",
    "    data, paragraph = get_annotations_data(file)\n",
    "    for doc in data:\n",
    "        # # Split text in chunks\n",
    "        # text = doc['context']\n",
    "        # split_list = text.split(sep='\\n \\n')\n",
    "        # split_list_clean = []\n",
    "        # for split in split_list:\n",
    "        #     if len(split.strip())==0:\n",
    "        #         pass\n",
    "        #     else:\n",
    "        #         split_list_clean.append(split)\n",
    "        # # Check topics\n",
    "        # split_topics = []\n",
    "        # for split in split_list_clean:\n",
    "        #     result = classifier(split, candidate_labels, multi_label=True)\n",
    "        #     split_topics.append(result['labels'][0])\n",
    "        # # Put together all split with same topic\n",
    "        # context_by_topic = {}\n",
    "        # for label in candidate_labels:\n",
    "        #     to_keep = [i for i, x in enumerate(split_topics) if x==label]\n",
    "        #     # Merge split with same topic\n",
    "        #     text = ''\n",
    "        #     for el in to_keep:\n",
    "        #         text+=split_list_clean[el]\n",
    "        #     context_by_topic[label] = text\n",
    "        for i, q in my_questions:\n",
    "            # print(q)\n",
    "            # print(doc['context'])\n",
    "            QA_input = {\n",
    "                'question': q,\n",
    "                'context': doc['context'] # Process complete text, TEST03\n",
    "            }\n",
    "            pred_answers[j].append((i, nlp(QA_input)))\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k, v in context_by_topic.items():\n",
    "#     print('*'*50)\n",
    "#     print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Total time = {end-start:.2f} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "daOFZmIcKT2s",
    "outputId": "3c5b98e6-91d4-4e78-e668-873cc121a9ec"
   },
   "outputs": [],
   "source": [
    "pred_ans_for_stats = [[] for i in repeat(None, len(all_annotations))]\n",
    "correct_ans_for_stats = [[] for i in repeat(None, len(all_annotations))]\n",
    "for k, file in enumerate(all_annotations):\n",
    "    counter = 0\n",
    "    data, paragraph = get_annotations_data(file)\n",
    "    for j, doc in enumerate(data):    \n",
    "        # print(f'****************** CONTEXT #{j} ******************\\n')\n",
    "        # print(f'{doc[\"context\"]}')\n",
    "        # PREDICTED ANSWERS\n",
    "        for i, q in my_questions:\n",
    "            pred_ans_for_stats[k].append(pred_answers[k][counter][1][\"answer\"].lower().strip(string.punctuation+' '))\n",
    "            correct_ans_for_stats[k].append(doc[f\"answer_{i}\"].lower().strip(string.punctuation+' '))\n",
    "            counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare correct answer with predicted one\n",
    "for i, (ref_list, pred_list) in enumerate(zip(correct_ans_for_stats, pred_ans_for_stats)):\n",
    "    print('#'*100)\n",
    "    print(f'Doc {all_annotations[i].split(\"/\")[-1]}')\n",
    "    for j, (ref, pred) in enumerate(zip(ref_list, pred_list)):\n",
    "        print('-'*50)\n",
    "        print(f'Question {my_questions[j][0]}: {my_questions[j][1]}')\n",
    "        print(f'Reference answer = \"{ref}\"')\n",
    "        print(f'Predicted answer = \"{pred}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Elaborate results by question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate EM Scores\n",
    "EM_scores = [[] for i in repeat(None, len(all_annotations))]\n",
    "EM_score_not_emptys = [[] for i in repeat(None, len(all_annotations))]\n",
    "EM_score_emptys = [[] for i in repeat(None, len(all_annotations))]\n",
    "for i, (correct_answers, pred_answers) in enumerate(zip(correct_ans_for_stats, pred_ans_for_stats)):\n",
    "    # print(i)\n",
    "    # EM_scores[i], EM_score_not_emptys[i], EM_score_emptys[i] = get_EM_score(correct_answers, pred_answers)\n",
    "    EM_scores[i], EM_score_not_emptys[i] = get_EM_score(correct_answers, pred_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate FPs, FNs, TPs\n",
    "all_TP_global = [[] for i in repeat(None, len(all_annotations))]\n",
    "all_FP_global = [[] for i in repeat(None, len(all_annotations))]\n",
    "all_FN_global = [[] for i in repeat(None, len(all_annotations))]\n",
    "for i, (correct_answers, pred_answers) in enumerate(zip(correct_ans_for_stats, pred_ans_for_stats)):\n",
    "    all_TP_global[i], all_FP_global[i], all_FN_global[i] = get_all_quest_param(correct_answers, pred_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1_global = [[] for i in repeat(None, len(all_annotations))]\n",
    "for i, (doc_TP, doc_FP, doc_FN) in enumerate(zip(all_TP_global, all_FP_global, all_FN_global)):\n",
    "    temp_FP = []\n",
    "    for j, (TP, FP, FN) in enumerate(zip(doc_TP, doc_FP, doc_FN)):\n",
    "        P, R, F1 = calculate_stat_param(TP, FP, FN)\n",
    "        temp_FP.append(F1)\n",
    "        F1_global[i] = temp_FP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregated results\n",
    "EM_TOTAL = statistics.mean(EM_scores)\n",
    "EM_NOT_EMPTY_TOTAL = statistics.mean(EM_score_not_emptys)\n",
    "full_TP = sum([sum(i) for i in zip(*all_TP_global)])\n",
    "full_FP = sum([sum(i) for i in zip(*all_FP_global)])\n",
    "full_FN = sum([sum(i) for i in zip(*all_FN_global)])\n",
    "F1_MACRO_TOTAL = statistics.mean([item for sublist in F1_global for item in sublist])\n",
    "P_TOTAL, R_TOTAL, F1_WEIGHTED_TOTAL = calculate_stat_param(full_TP, full_FP, full_FN)\n",
    "\n",
    "print(f'GLOBAL EM = {EM_TOTAL*100:.2f}%')\n",
    "print(f'GLOBAL EM (NOT EMPTY ANSWERS) = {EM_NOT_EMPTY_TOTAL*100:.2f}%')\n",
    "print(f'GLOBAL P = {P_TOTAL*100:.2f}%')\n",
    "print(f'GLOBAL R = {R_TOTAL*100:.2f}%')\n",
    "print(f'GLOBAL F1 WEIGHTED = {F1_WEIGHTED_TOTAL*100:.2f}%')\n",
    "print(f'GLOBAL F1 MACRO = {F1_MACRO_TOTAL*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Single Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single question\n",
    "TPs = [0] * len(my_questions)\n",
    "FPs = [0] * len(my_questions)\n",
    "FNs = [0] * len(my_questions)\n",
    "EMs = [0] * len(my_questions)\n",
    "quest_num = len(my_questions)\n",
    "for i, stats in enumerate(zip(all_TP_global, all_FP_global, all_FN_global)):\n",
    "    tp, fp, fn = stats[0], stats[1], stats[2]\n",
    "    for j in range(quest_num):\n",
    "        TPs[j]+=tp[j]\n",
    "        FPs[j]+=fp[j]\n",
    "        FNs[j]+=fn[j]\n",
    "        # if (i-j)%quest_num==0:\n",
    "        #     TPs[j]+=tp[j]\n",
    "        #     FPs[j]+=fp[j]\n",
    "        #     FNs[j]+=fn[j]\n",
    "        if correct_ans_for_stats[i] == pred_ans_for_stats[i]:\n",
    "        # if correct_ans_for_stats[i].lower().strip() == pred_ans_for_stats[i].lower().strip():\n",
    "            EMs[j]+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = []\n",
    "for i, q in my_questions:\n",
    "    index.append(f'Q{i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions = pd.DataFrame(\n",
    "    index=index,\n",
    "    columns=['Precision', 'Recall', 'F1-score', 'EM']\n",
    ")\n",
    "\n",
    "counter = 0\n",
    "for i, q in my_questions:\n",
    "    # print(i, q)\n",
    "    P, R, F1 = calculate_stat_param(TPs[counter], FPs[counter], FNs[counter])\n",
    "    # print(P, R, F1)\n",
    "    try:\n",
    "        df_questions.loc[f'Q{i}', 'Precision']=f'{P*100:.2f}'\n",
    "    except:\n",
    "        df_questions.loc[f'Q{i}', 'Precision']='0'\n",
    "    try:\n",
    "        df_questions.loc[f'Q{i}', 'Recall']=f'{R*100:.2f}'\n",
    "    except:\n",
    "        df_questions.loc[f'Q{i}', 'Recall']='0'\n",
    "    try:\n",
    "        df_questions.loc[f'Q{i}', 'F1-score']=f'{F1*100:.2f}'\n",
    "    except:\n",
    "        df_questions.loc[f'Q{i}', 'F1-score']='0'\n",
    "    try:\n",
    "        if (P==1 and R==1):\n",
    "            em = 100\n",
    "        else:\n",
    "            em = 0\n",
    "        df_questions.loc[f'Q{i}', 'EM']=em\n",
    "    except:\n",
    "        pass\n",
    "    counter+=1\n",
    "df_questions = df_questions.apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1-score\n",
    "# Convert dataframe to heatmap, by questions\n",
    "sns.set(font_scale=2)\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "sns.heatmap(df_questions, cmap='RdYlGn', annot=True, fmt=\".2f\", annot_kws={\"fontsize\":20}, vmin=0, vmax=100)\n",
    "fig.savefig(f'QUESTIONS_heat.png', dpi=400, transparent=True, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Elaborate results by document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Single document\n",
    "# step = 5\n",
    "# tot = len(all_TP)/step\n",
    "\n",
    "# df_documents = pd.DataFrame(\n",
    "#     # index=['Q1', 'Q2', 'Q3', 'Q4', 'Q5'],\n",
    "#     columns=['Precision', 'Recall', 'F1-score']\n",
    "# )\n",
    "\n",
    "# em_documents = pd.DataFrame(\n",
    "#     columns=['F1-score']\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for el in range(int(tot)):\n",
    "#     # print('*'*50)\n",
    "#     # print(el)\n",
    "#     split_TP = all_TP[el*5:(el+1)*5]\n",
    "#     TP_doc = sum(split_TP)\n",
    "#     split_FP = all_FP[el*5:(el+1)*5]\n",
    "#     FP_doc = sum(split_FP)\n",
    "#     split_FN = all_FN[el*5:(el+1)*5]\n",
    "#     FN_doc = sum(split_FN)\n",
    "#     P, R, F = calculate_stat_param(TP_doc, FP_doc, FN_doc)\n",
    "#     # print(f'DOCUMENT {el+1} PRECISION = {P*100:.2f}%')\n",
    "#     # print(f'DOCUMENT {el+1} RECALL = {R*100:.2f}%')\n",
    "#     # Calculate EM\n",
    "#     split_pred_ans = pred_ans_for_stats[el*5:(el+1)*5]\n",
    "#     split_ref_ans = correct_ans_for_stats[el*5:(el+1)*5]\n",
    "#     em_count = 0\n",
    "#     for pred, ref in zip(split_pred_ans, split_ref_ans):\n",
    "#         if pred==ref:\n",
    "#             em_count+=1\n",
    "#     try:\n",
    "#         # print(f'DOCUMENT {el+1} F1-SCORE = {F*100:.2f}%')\n",
    "#         df_documents.loc[el] = [f'{P*100:.2f}', f'{R*100:.2f}', f'{F*100:.2f}']\n",
    "#     except:\n",
    "#         # print(f'DOCUMENT {el+1} F1-SCORE = {F}')\n",
    "#         df_documents.loc[el] = [f'{P*100:.2f}', f'{R*100:.2f}', 0]\n",
    "#         F = 0\n",
    "#     df_documents.loc[el] = [f'{P*100:.2f}', f'{R*100:.2f}', f'{F*100:.2f}']\n",
    "#     em_documents.loc[el] = [f'{em_count/5*100:.2f}']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_documents = df_documents.apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_len = []\n",
    "# for j, doc in enumerate(data):    \n",
    "#     doc_len.append(len(doc[\"context\"]))\n",
    "#     # if j==6:\n",
    "#     #     print(doc[\"context\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_documents = pd.concat([df_documents, pd.Series(doc_len, name='Doc lenght')], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_documents.to_csv('df_documents.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.set(font_scale=2)\n",
    "# fig, ax = plt.subplots(figsize=(10,10))\n",
    "# sns.heatmap(df_documents.iloc[0:10,:3], cmap='RdYlGn', annot=True, fmt=\".2f\", annot_kws={\"fontsize\":20}, vmin=0, vmax=100)\n",
    "# fig.savefig(f'DOCUMENTS_1_heat.png', dpi=400, transparent=True, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.set(font_scale=2)\n",
    "# fig, ax = plt.subplots(figsize=(10,10))\n",
    "# sns.heatmap(df_documents.iloc[10:20,:3], cmap='RdYlGn', annot=True, fmt=\".2f\", annot_kws={\"fontsize\":20}, vmin=0, vmax=100)\n",
    "# fig.savefig(f'DOCUMENTS_2_heat.png', dpi=400, transparent=True, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sns.set(font_scale=2)\n",
    "# fig, ax = plt.subplots(figsize=(10,10))\n",
    "# sns.heatmap(df_documents.iloc[20:30,:3], cmap='RdYlGn', annot=True, fmt=\".2f\", annot_kws={\"fontsize\":20}, vmin=0, vmax=100)\n",
    "# fig.savefig(f'DOCUMENTS_3_heat.png', dpi=400, transparent=True, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize=(10,10))\n",
    "# plt.scatter(df_documents['Doc lenght'].tolist(), df_documents['F1-score'].tolist())\n",
    "# # plt.scatter(df_documents['Doc lenght'][:4].tolist(), df_documents['F1-score'][:4].tolist())#, marker='^')\n",
    "# fig.savefig(f'DocLen_VS_F1Score', dpi=400, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Plot and interpolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scipy as sp\n",
    "# from scipy.interpolate import interp1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def log_interp1d(xx, yy, kind='linear'):\n",
    "#     logx = np.log10(xx)\n",
    "#     logy = np.log10(yy)\n",
    "#     lin_interp = sp.interpolate.interp1d(logx, logy, kind=kind)\n",
    "#     log_interp = lambda zz: np.power(10.0, lin_interp(np.log10(zz)))\n",
    "#     return log_interp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sort Dataframe\n",
    "# df_documents =  df_documents.sort_values(by='Doc lenght')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = np.array(df_documents['Doc lenght'].tolist())\n",
    "# y = np.array(df_documents['F1-score'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # f = interp1d(x, y)\n",
    "# # f2 = interp1d(x, y, kind='quadratic')\n",
    "# f2 = log_interp1d(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xnew = np.arange(x[0], x[-1], (x[-1]-x[0])/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize=(10,10))\n",
    "# # plt.plot(x, y, 'o', xnew, f2(xnew), '--')\n",
    "# plt.plot(x, y, 'o', xnew, f2(xnew), '--')\n",
    "# plt.legend(['data', 'interp'], loc='best')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Elaborate global results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_TP = sum(all_TP)\n",
    "# total_FP = sum(all_FP)\n",
    "# total_FN = sum(all_FN)\n",
    "\n",
    "# total_precision = total_TP/(total_TP+total_FP)\n",
    "# total_recall = total_TP/(total_TP+total_FN)\n",
    "# total_f1 = 2*total_precision*total_recall/(total_precision+total_recall)\n",
    "\n",
    "# print(f'Total Precision = {total_precision*100:.2f}%')\n",
    "# print(f'Total Recall = {total_recall*100:.2f}%')\n",
    "# print(f'Total F1-score = {total_f1*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Calculate BLEU score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_corpus_bleu = [[i.split()] for i in correct_ans_for_stats]\n",
    "pred_corpus_bleu = [i.split() for i in pred_ans_for_stats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_ans_for_stats[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ans_for_stats[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = [[\n",
    "    'this is a dog'.split(),\n",
    "    'it is dog'.split(),\n",
    "    'dog it is'.split(),\n",
    "    'a dog, it is'.split() \n",
    "]]\n",
    "reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate = ['it is dog'.split()]\n",
    "candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLEU score of all answers\n",
    "# score = corpus_bleu(ref_corpus_bleu, pred_corpus_bleu)\n",
    "# score = sentence_bleu(reference, candidate)\n",
    "score = corpus_bleu(reference, candidate)\n",
    "print(f'Total BLEU score = {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "hypothesis = ['This', 'is', 'cat'] \n",
    "reference = ['This', 'is', 'a', 'cat']\n",
    "references = [reference] # list of references for 1 sentence.\n",
    "list_of_references = [references] # list of references for all sentences in corpus.\n",
    "list_of_hypotheses = [hypothesis] # list of hypotheses that corresponds to list of references.\n",
    "nltk.translate.bleu_score.corpus_bleu(list_of_references, list_of_hypotheses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference answers, divided by question\n",
    "ref_corpus_bleu_Q1 = [[i.split()] for j, i in enumerate(correct_ans_for_stats) if j%5==0]\n",
    "ref_corpus_bleu_Q2 = [[i.split()] for j, i in enumerate(correct_ans_for_stats) if (j-1)%5==0]\n",
    "ref_corpus_bleu_Q3 = [[i.split()] for j, i in enumerate(correct_ans_for_stats) if (j-2)%5==0]\n",
    "ref_corpus_bleu_Q4 = [[i.split()] for j, i in enumerate(correct_ans_for_stats) if (j-3)%5==0]\n",
    "ref_corpus_bleu_Q5 = [[i.split()] for j, i in enumerate(correct_ans_for_stats) if (j-4)%5==0]\n",
    "ref_corpus_TOT = [ref_corpus_bleu_Q1, ref_corpus_bleu_Q2, ref_corpus_bleu_Q3, ref_corpus_bleu_Q4, ref_corpus_bleu_Q5]\n",
    "\n",
    "# Predicted answers, divided by question\n",
    "pred_corpus_bleu_Q1 = [i.split() for j, i in enumerate(pred_ans_for_stats) if j%5==0]\n",
    "pred_corpus_bleu_Q2 = [i.split() for j, i in enumerate(pred_ans_for_stats) if (j-1)%5==0]\n",
    "pred_corpus_bleu_Q3 = [i.split() for j, i in enumerate(pred_ans_for_stats) if (j-2)%5==0]\n",
    "pred_corpus_bleu_Q4 = [i.split() for j, i in enumerate(pred_ans_for_stats) if (j-3)%5==0]\n",
    "pred_corpus_bleu_Q5 = [i.split() for j, i in enumerate(pred_ans_for_stats) if (j-4)%5==0]\n",
    "pred_corpus_TOT = [pred_corpus_bleu_Q1, pred_corpus_bleu_Q2, pred_corpus_bleu_Q3, pred_corpus_bleu_Q4, pred_corpus_bleu_Q5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for j, (ref, pred) in enumerate(zip(ref_corpus_TOT, pred_corpus_TOT)):\n",
    "#     score = corpus_bleu(ref, pred)\n",
    "#     print(f'Total BLEU score = {score:.3f} for Question {j+1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores = []\n",
    "# for target, prediction in zip(correct_ans_for_stats, pred_ans_for_stats):\n",
    "#     print('^'*50)\n",
    "#     print(f'Reference = \"{target}\"')\n",
    "#     print(f'Prediction = \"{prediction}\"')\n",
    "#     print(f'BLEU SCORE = {sentence_bleu([target], prediction):.3f}')\n",
    "#     scores.append(sentence_bleu([target], prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HoLVPN8N7SSk"
   },
   "outputs": [],
   "source": [
    "# model_checkpoint = 'deepset/roberta-base-squad2' # SQUAD2 multilangual model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P2TkdlHfAz50"
   },
   "outputs": [],
   "source": [
    "# # Made-up context, short, translated with Google translate\n",
    "# my_context = '''70-year-old male patient. Examination performed urgently, in basal conditions.\n",
    "# In the depth of the white matter in the right temporo-parietal site we can appreciate furnishing\n",
    "# roughly roundish of about 2 cm surrounded by an imposing edema reaction.\n",
    "# Another more minute hypodense area of the white matter seems to be appreciated in the frontal seat since.\n",
    "# There is currently no evidence of intracranial bleeding.\n",
    "# Median structure in axis.\n",
    "# Ventricular complex of normal amplitude and morphology.\n",
    "# An in-depth diagnostic study could be appropriate, possibly confirming with an MRI examination\n",
    "# Therefore the patient resigns with the following advice:\n",
    "# - Keppra 500 1 tablet twice a day;\n",
    "# - Soldesam 4 mg 1 fl im per day for 15 days; then deltacortene 25 mg 1 tablet twice a day for 5 days; subsequently 1 tablet per day for a further 5 days;\n",
    "# - Antra 20 mg 1 tablet / day for 20 days;\n",
    "# - Brain MRI in 30 days and subsequent neurological re-evaluation to decide on a further treatment plan.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NOg_4yV1L_D4"
   },
   "outputs": [],
   "source": [
    "# # Made-up context, long, translated with Google translate\n",
    "# my_context = '''Dear colleague, your client resigns on: 16.06.2015\n",
    "\n",
    "# with the following diagnosis:\n",
    "# DIASTOLIC CARDIAC INSUFFICIENCY WITH DILATATION OF THE DX SECTIONS IN FA IN TAO IN LABYLE COMPENSATION (CONGESTIZIO DISOMPENSATION = EDEMI OF CONSPICIOUS LOWER LIMBS)) WITH PULMONARY HYPERTENSION. ULCERS FROM INFECTED LOWER LIMBS PHLEBOSTASIS (PSEUDOMONAS AERUGINOSA). SEVERAL RIGHT GONARTHROSIS SUBJECTED TO ARTHROCENTHESIS. TYPE 2 DIABETES MELLITUS REWARDED AND COMPLICATED BY NEPHROPATHY. CHRONIC KIDNEY INSUFFICIENCY REGREDITY.\n",
    " \n",
    "# Anamnestic news:\n",
    "# Non-smoking patient, suffering from arterial hypertension, type 2 diabetes mellitus, permanent atrial fibrillation in NAO. Denies allergies. Following the onset of hyperpyrexia unresponsive to antibiotic polytherapy (clarithromycin, piperacillin / tazobactam) and progressive pain in the right knee limiting walking, she arrives in the emergency room where she performs a radiogram examination of the right knee and orthopedic examination which, due to a suspected septic arthritis, recommends hospitalization in an internist setting for evaluations and appropriate treatments.\n",
    "\n",
    "# Abnormal blood tests on discharge:\n",
    "# Glycosylated Hb 8.2; GFR: 85ml / min; Creatinine: 0.7mg; BUN: 23.0mg, pro-BNP: 3139.0; Microalbuminuria 60.5; Sodium 132 -> 135; blood sugar 210 -> 41; Sideremia 42; Alkaline phosphatase 110; LDH 233; Hb 12.5; pro-BNP 5103; Albumin 38.9; Alpha-1-globulins 7,1; Alpha-2-globulins 14.8; Beta-2-globulins 6,7; Gamma-globulins 44.3; ESR 52; PCR: 5.18, e.g. right skin ulcer swab: Positive for Ciprofloxacin sensitive Pseudomonas; ex. biological liquid: 35200 cells, Cell. prevalence of polymormonucleated, LDH: 1330.0, Prot.Tot: 5.00, ex. cultural: neg.\n",
    "   \n",
    "\n",
    "# Instrumental examinations:\n",
    "# RX OF THE CHEST - RX OF THE RIGHT KNEE (4/06/2015): Marked tricompartmental gonarthrosis phenomena on the right. Marked swelling of the soft tissues of the right knee, particularly evident in the supra-patellar site as due to the presence of an effusion in the sub-quadriceps synovial recess. Diffuse irregular femoro-popliteal-tibio-peroneal vascular calcifications. Osteopenia. The examination of the thorax does not show pulmonary consolidations in the act of inflammatory significance or pleural effusion flaps. Enlarged cardiac image. Signs of stagnation in the small circle. Aorta ectatic and calcific.\n",
    "# ELECTROCARDIOGRAM (5/06/2015): Atrial fibrillation at 82 bpm; BBDx almost complete. Partially secondary anomalies of ventricular repolarization.\n",
    "# RHEUMATOLOGICAL EXAMINATION (7/06/2015): About 8 days ago onset of fever and right knee arthralgia with joint swelling. No joint pain at rest. Not fever at present. OE: signs of joint effusion of the right knee, non rubor, functional limitation of the right knee joint, slight edema of the right leg. Recent right leg ulcer infection. In anticoagulant therapy. Conclusion: probable right knee hemarthrosis in severe gonarthrosis. We recommend: ice pack for 20 minutes for 2 vv / day. Rest of the lower limbs. Monitor PCR, protein electrophoresis every 5 days. ETG knee.\n",
    "# ECHOCARDIOGRAM (10/06/2015): Examination performed in the patient's bed, not very cooperative. With these limitations we document: Ritmo da F.A. Slightly hypertrophic left ventricle (septal thickness 11.4 mm), not dilated, with global contractile function at the lower limits of the norm (EF about 50%). Bi-atrial dilations, greater on the right. Mild to moderate mitral regurgitation. Mild aortic regurgitation, in sclerotic valve. Right ventricle dilated and slightly hypokinetic (TAPSE 17 mm). Moderate-severe IT with PAPs estimated at 25 mmHg + 15 mmHg. VCI dilated and hypocollassing (27/20 mm).\n",
    "\n",
    "# Observations / Therapies carried out:\n",
    "# During the hospital stay, the patient underwent arthrocentesis (17.0718) of the right knee with sampling of synovial fluid; subjected to empirical antibiotic therapy first and then targeted (ATB) we witnessed a regression of the fever and clinical improvement and skin ulcers of the lower limbs; also subjected to t. diuretic and iv edema of the lower limbs almost regressed.\n",
    "\n",
    "# Home treatment and indications:\n",
    "# HYPOSODIC AND HYPOCALORIC DIET\n",
    "#     • Eucreas 50 mg / 1000 mg 1 tablet for 2 vv / day\n",
    "#     • Baypress 20 mg - 1 tablet / day\n",
    "#     • Eliquis 2.5 1 cp for 2 vv / day\n",
    "#     • Carvedilol 6.25 mg - 1 tablet / day\n",
    "#     • Lasix 25 mg - 1 tablet for 2 vv / day\n",
    "#     • Luvion 50 mg - 1 tablet / day\n",
    "#     • Ciproxin 500 mg - 1 tablet for 2 vv / day for 10 days\n",
    "#     • Lower limb skin dressing right with phytostimolines\n",
    "\n",
    "\n",
    "# Also perform:\n",
    "#     • Kidney function, protein electrophoresis, PCR in about 5-7 days\n",
    "#     • Cons. Rheumatology + ETG right knee in 15 days\n",
    "\n",
    "# Periodically check blood pressure, body weight, kidney and liver function tests, blood counts.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t-eCyoOSAz50"
   },
   "outputs": [],
   "source": [
    "# my_questions = [\n",
    "#                 'What is the pathological condition?',\n",
    "#                 'What is the age?',\n",
    "#                 'What is the sex?',\n",
    "#                 'What medications he takes?',\n",
    "#                 'What are the surgical procedures to which he is subjected?',\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-gACHUmoBaLI"
   },
   "outputs": [],
   "source": [
    "# # a) Get predictions\n",
    "# nlp = pipeline('question-answering', model=model_checkpoint, tokenizer=model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B__f51ALH0Az"
   },
   "outputs": [],
   "source": [
    "# my_answers = []\n",
    "\n",
    "# for q in my_questions:\n",
    "#   QA_input = {\n",
    "#       'question': q,\n",
    "#       'context': my_context\n",
    "#   }\n",
    "#   my_answers.append(nlp(QA_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oUYfTdXXBcqG",
    "outputId": "9111b6c2-09f6-4c54-bf53-95c001f21d14"
   },
   "outputs": [],
   "source": [
    "# print(f'****************** CONTEXT ******************\\n{my_context}')\n",
    "# for i, (q, ans) in enumerate(zip((my_questions), my_answers)):\n",
    "#   print('*'*50)\n",
    "#   print(f'QUESTION INDEX {i+1}= \"{q}\"')\n",
    "#   print(f'ANSWER = \"{ans[\"answer\"]}\"')\n",
    "#   print(f'ANSWER SCORE = {ans[\"score\"]*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JKBmVxNQCVHc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "QA_Bot_01.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
